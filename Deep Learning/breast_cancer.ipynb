{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "data_std = ss.fit_transform(bc.data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_std, bc.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = Input((30,))\n",
    "x = Dense(1, activation=\"sigmoid\")(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(data_input, x)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 455 samples, validate on 114 samples\n",
      "Epoch 1/200\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.0868 - acc: 0.9890 - val_loss: 0.0896 - val_acc: 0.9825\n",
      "Epoch 2/200\n",
      "455/455 [==============================] - 0s 38us/step - loss: 0.0866 - acc: 0.9890 - val_loss: 0.0894 - val_acc: 0.9825\n",
      "Epoch 3/200\n",
      "455/455 [==============================] - 0s 41us/step - loss: 0.0865 - acc: 0.9890 - val_loss: 0.0893 - val_acc: 0.9825\n",
      "Epoch 4/200\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.0864 - acc: 0.9890 - val_loss: 0.0892 - val_acc: 0.9825\n",
      "Epoch 5/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0862 - acc: 0.9890 - val_loss: 0.0891 - val_acc: 0.9825\n",
      "Epoch 6/200\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.0861 - acc: 0.9890 - val_loss: 0.0890 - val_acc: 0.9825\n",
      "Epoch 7/200\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.0860 - acc: 0.9890 - val_loss: 0.0889 - val_acc: 0.9825\n",
      "Epoch 8/200\n",
      "455/455 [==============================] - 0s 43us/step - loss: 0.0858 - acc: 0.9890 - val_loss: 0.0888 - val_acc: 0.9825\n",
      "Epoch 9/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0857 - acc: 0.9890 - val_loss: 0.0887 - val_acc: 0.9825\n",
      "Epoch 10/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0856 - acc: 0.9890 - val_loss: 0.0886 - val_acc: 0.9825\n",
      "Epoch 11/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0854 - acc: 0.9890 - val_loss: 0.0885 - val_acc: 0.9825\n",
      "Epoch 12/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0853 - acc: 0.9890 - val_loss: 0.0884 - val_acc: 0.9825\n",
      "Epoch 13/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0852 - acc: 0.9890 - val_loss: 0.0883 - val_acc: 0.9825\n",
      "Epoch 14/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0850 - acc: 0.9890 - val_loss: 0.0882 - val_acc: 0.9825\n",
      "Epoch 15/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0849 - acc: 0.9890 - val_loss: 0.0880 - val_acc: 0.9825\n",
      "Epoch 16/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0848 - acc: 0.9890 - val_loss: 0.0879 - val_acc: 0.9825\n",
      "Epoch 17/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0847 - acc: 0.9890 - val_loss: 0.0878 - val_acc: 0.9825\n",
      "Epoch 18/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0845 - acc: 0.9890 - val_loss: 0.0877 - val_acc: 0.9825\n",
      "Epoch 19/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0844 - acc: 0.9890 - val_loss: 0.0876 - val_acc: 0.9825\n",
      "Epoch 20/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0843 - acc: 0.9890 - val_loss: 0.0875 - val_acc: 0.9825\n",
      "Epoch 21/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0842 - acc: 0.9890 - val_loss: 0.0874 - val_acc: 0.9825\n",
      "Epoch 22/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0840 - acc: 0.9890 - val_loss: 0.0873 - val_acc: 0.9825\n",
      "Epoch 23/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0839 - acc: 0.9890 - val_loss: 0.0873 - val_acc: 0.9825\n",
      "Epoch 24/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0838 - acc: 0.9890 - val_loss: 0.0872 - val_acc: 0.9825\n",
      "Epoch 25/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0837 - acc: 0.9890 - val_loss: 0.0871 - val_acc: 0.9825\n",
      "Epoch 26/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0836 - acc: 0.9890 - val_loss: 0.0870 - val_acc: 0.9825\n",
      "Epoch 27/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0834 - acc: 0.9890 - val_loss: 0.0869 - val_acc: 0.9825\n",
      "Epoch 28/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0833 - acc: 0.9890 - val_loss: 0.0868 - val_acc: 0.9825\n",
      "Epoch 29/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0832 - acc: 0.9890 - val_loss: 0.0867 - val_acc: 0.9825\n",
      "Epoch 30/200\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0831 - acc: 0.9890 - val_loss: 0.0867 - val_acc: 0.9825\n",
      "Epoch 31/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0829 - acc: 0.9890 - val_loss: 0.0866 - val_acc: 0.9825\n",
      "Epoch 32/200\n",
      "455/455 [==============================] - 0s 43us/step - loss: 0.0828 - acc: 0.9890 - val_loss: 0.0865 - val_acc: 0.9825\n",
      "Epoch 33/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0827 - acc: 0.9890 - val_loss: 0.0864 - val_acc: 0.9825\n",
      "Epoch 34/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0826 - acc: 0.9890 - val_loss: 0.0863 - val_acc: 0.9825\n",
      "Epoch 35/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0825 - acc: 0.9890 - val_loss: 0.0863 - val_acc: 0.9825\n",
      "Epoch 36/200\n",
      "455/455 [==============================] - 0s 43us/step - loss: 0.0824 - acc: 0.9890 - val_loss: 0.0862 - val_acc: 0.9825\n",
      "Epoch 37/200\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.0823 - acc: 0.9890 - val_loss: 0.0861 - val_acc: 0.9825\n",
      "Epoch 38/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0821 - acc: 0.9890 - val_loss: 0.0861 - val_acc: 0.9825\n",
      "Epoch 39/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0820 - acc: 0.9890 - val_loss: 0.0860 - val_acc: 0.9825\n",
      "Epoch 40/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0819 - acc: 0.9890 - val_loss: 0.0860 - val_acc: 0.9825\n",
      "Epoch 41/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0818 - acc: 0.9890 - val_loss: 0.0859 - val_acc: 0.9825\n",
      "Epoch 42/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0817 - acc: 0.9890 - val_loss: 0.0858 - val_acc: 0.9825\n",
      "Epoch 43/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0816 - acc: 0.9890 - val_loss: 0.0857 - val_acc: 0.9825\n",
      "Epoch 44/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0815 - acc: 0.9890 - val_loss: 0.0856 - val_acc: 0.9825\n",
      "Epoch 45/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0814 - acc: 0.9890 - val_loss: 0.0855 - val_acc: 0.9825\n",
      "Epoch 46/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0813 - acc: 0.9890 - val_loss: 0.0854 - val_acc: 0.9825\n",
      "Epoch 47/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0812 - acc: 0.9890 - val_loss: 0.0854 - val_acc: 0.9825\n",
      "Epoch 48/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0810 - acc: 0.9890 - val_loss: 0.0852 - val_acc: 0.9825\n",
      "Epoch 49/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0809 - acc: 0.9890 - val_loss: 0.0852 - val_acc: 0.9825\n",
      "Epoch 50/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0808 - acc: 0.9890 - val_loss: 0.0851 - val_acc: 0.9825\n",
      "Epoch 51/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0807 - acc: 0.9890 - val_loss: 0.0850 - val_acc: 0.9825\n",
      "Epoch 52/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0806 - acc: 0.9890 - val_loss: 0.0850 - val_acc: 0.9825\n",
      "Epoch 53/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0805 - acc: 0.9890 - val_loss: 0.0849 - val_acc: 0.9825\n",
      "Epoch 54/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0804 - acc: 0.9890 - val_loss: 0.0848 - val_acc: 0.9825\n",
      "Epoch 55/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0803 - acc: 0.9890 - val_loss: 0.0847 - val_acc: 0.9825\n",
      "Epoch 56/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0802 - acc: 0.9890 - val_loss: 0.0847 - val_acc: 0.9825\n",
      "Epoch 57/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0801 - acc: 0.9890 - val_loss: 0.0846 - val_acc: 0.9825\n",
      "Epoch 58/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0800 - acc: 0.9890 - val_loss: 0.0845 - val_acc: 0.9825\n",
      "Epoch 59/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0799 - acc: 0.9890 - val_loss: 0.0845 - val_acc: 0.9825\n",
      "Epoch 60/200\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0798 - acc: 0.9890 - val_loss: 0.0844 - val_acc: 0.9825\n",
      "Epoch 61/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0797 - acc: 0.9890 - val_loss: 0.0843 - val_acc: 0.9825\n",
      "Epoch 62/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0796 - acc: 0.9890 - val_loss: 0.0842 - val_acc: 0.9825\n",
      "Epoch 63/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0795 - acc: 0.9890 - val_loss: 0.0841 - val_acc: 0.9825\n",
      "Epoch 64/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0794 - acc: 0.9890 - val_loss: 0.0840 - val_acc: 0.9825\n",
      "Epoch 65/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0793 - acc: 0.9890 - val_loss: 0.0840 - val_acc: 0.9825\n",
      "Epoch 66/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0792 - acc: 0.9890 - val_loss: 0.0839 - val_acc: 0.9825\n",
      "Epoch 67/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0791 - acc: 0.9890 - val_loss: 0.0838 - val_acc: 0.9825\n",
      "Epoch 68/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0790 - acc: 0.9890 - val_loss: 0.0837 - val_acc: 0.9825\n",
      "Epoch 69/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0789 - acc: 0.9890 - val_loss: 0.0837 - val_acc: 0.9825\n",
      "Epoch 70/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0788 - acc: 0.9890 - val_loss: 0.0836 - val_acc: 0.9825\n",
      "Epoch 71/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0787 - acc: 0.9890 - val_loss: 0.0835 - val_acc: 0.9825\n",
      "Epoch 72/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0786 - acc: 0.9890 - val_loss: 0.0834 - val_acc: 0.9825\n",
      "Epoch 73/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0785 - acc: 0.9890 - val_loss: 0.0833 - val_acc: 0.9825\n",
      "Epoch 74/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0784 - acc: 0.9890 - val_loss: 0.0833 - val_acc: 0.9825\n",
      "Epoch 75/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0783 - acc: 0.9890 - val_loss: 0.0832 - val_acc: 0.9825\n",
      "Epoch 76/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0782 - acc: 0.9890 - val_loss: 0.0831 - val_acc: 0.9825\n",
      "Epoch 77/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0781 - acc: 0.9890 - val_loss: 0.0830 - val_acc: 0.9825\n",
      "Epoch 78/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0780 - acc: 0.9890 - val_loss: 0.0829 - val_acc: 0.9825\n",
      "Epoch 79/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0779 - acc: 0.9890 - val_loss: 0.0829 - val_acc: 0.9825\n",
      "Epoch 80/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0778 - acc: 0.9890 - val_loss: 0.0828 - val_acc: 0.9825\n",
      "Epoch 81/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0778 - acc: 0.9890 - val_loss: 0.0828 - val_acc: 0.9825\n",
      "Epoch 82/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0777 - acc: 0.9890 - val_loss: 0.0827 - val_acc: 0.9825\n",
      "Epoch 83/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0776 - acc: 0.9890 - val_loss: 0.0826 - val_acc: 0.9825\n",
      "Epoch 84/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0775 - acc: 0.9890 - val_loss: 0.0826 - val_acc: 0.9825\n",
      "Epoch 85/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0774 - acc: 0.9890 - val_loss: 0.0825 - val_acc: 0.9825\n",
      "Epoch 86/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0773 - acc: 0.9890 - val_loss: 0.0825 - val_acc: 0.9825\n",
      "Epoch 87/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0772 - acc: 0.9890 - val_loss: 0.0824 - val_acc: 0.9825\n",
      "Epoch 88/200\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0771 - acc: 0.9890 - val_loss: 0.0823 - val_acc: 0.9825\n",
      "Epoch 89/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0770 - acc: 0.9890 - val_loss: 0.0823 - val_acc: 0.9825\n",
      "Epoch 90/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0769 - acc: 0.9890 - val_loss: 0.0822 - val_acc: 0.9825\n",
      "Epoch 91/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0768 - acc: 0.9890 - val_loss: 0.0821 - val_acc: 0.9825\n",
      "Epoch 92/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0767 - acc: 0.9890 - val_loss: 0.0821 - val_acc: 0.9825\n",
      "Epoch 93/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0767 - acc: 0.9890 - val_loss: 0.0820 - val_acc: 0.9825\n",
      "Epoch 94/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0766 - acc: 0.9890 - val_loss: 0.0820 - val_acc: 0.9825\n",
      "Epoch 95/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0765 - acc: 0.9890 - val_loss: 0.0819 - val_acc: 0.9825\n",
      "Epoch 96/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0764 - acc: 0.9890 - val_loss: 0.0818 - val_acc: 0.9825\n",
      "Epoch 97/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0763 - acc: 0.9890 - val_loss: 0.0817 - val_acc: 0.9825\n",
      "Epoch 98/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0762 - acc: 0.9890 - val_loss: 0.0816 - val_acc: 0.9825\n",
      "Epoch 99/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0762 - acc: 0.9890 - val_loss: 0.0816 - val_acc: 0.9825\n",
      "Epoch 100/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0761 - acc: 0.9890 - val_loss: 0.0815 - val_acc: 0.9825\n",
      "Epoch 101/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0760 - acc: 0.9890 - val_loss: 0.0815 - val_acc: 0.9825\n",
      "Epoch 102/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0759 - acc: 0.9890 - val_loss: 0.0814 - val_acc: 0.9825\n",
      "Epoch 103/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0758 - acc: 0.9890 - val_loss: 0.0814 - val_acc: 0.9825\n",
      "Epoch 104/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0757 - acc: 0.9890 - val_loss: 0.0814 - val_acc: 0.9825\n",
      "Epoch 105/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0757 - acc: 0.9890 - val_loss: 0.0813 - val_acc: 0.9825\n",
      "Epoch 106/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0756 - acc: 0.9890 - val_loss: 0.0813 - val_acc: 0.9825\n",
      "Epoch 107/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0755 - acc: 0.9890 - val_loss: 0.0812 - val_acc: 0.9825\n",
      "Epoch 108/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0754 - acc: 0.9890 - val_loss: 0.0812 - val_acc: 0.9825\n",
      "Epoch 109/200\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0753 - acc: 0.9890 - val_loss: 0.0811 - val_acc: 0.9825\n",
      "Epoch 110/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0752 - acc: 0.9890 - val_loss: 0.0811 - val_acc: 0.9825\n",
      "Epoch 111/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0752 - acc: 0.9890 - val_loss: 0.0810 - val_acc: 0.9825\n",
      "Epoch 112/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0751 - acc: 0.9890 - val_loss: 0.0809 - val_acc: 0.9825\n",
      "Epoch 113/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0750 - acc: 0.9890 - val_loss: 0.0809 - val_acc: 0.9825\n",
      "Epoch 114/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0749 - acc: 0.9890 - val_loss: 0.0808 - val_acc: 0.9825\n",
      "Epoch 115/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0748 - acc: 0.9890 - val_loss: 0.0808 - val_acc: 0.9825\n",
      "Epoch 116/200\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0748 - acc: 0.9890 - val_loss: 0.0808 - val_acc: 0.9825\n",
      "Epoch 117/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0747 - acc: 0.9890 - val_loss: 0.0807 - val_acc: 0.9825\n",
      "Epoch 118/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0746 - acc: 0.9890 - val_loss: 0.0806 - val_acc: 0.9825\n",
      "Epoch 119/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0745 - acc: 0.9890 - val_loss: 0.0806 - val_acc: 0.9825\n",
      "Epoch 120/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0744 - acc: 0.9890 - val_loss: 0.0805 - val_acc: 0.9825\n",
      "Epoch 121/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0744 - acc: 0.9890 - val_loss: 0.0805 - val_acc: 0.9825\n",
      "Epoch 122/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0743 - acc: 0.9890 - val_loss: 0.0804 - val_acc: 0.9825\n",
      "Epoch 123/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0742 - acc: 0.9890 - val_loss: 0.0803 - val_acc: 0.9825\n",
      "Epoch 124/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0741 - acc: 0.9890 - val_loss: 0.0803 - val_acc: 0.9825\n",
      "Epoch 125/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0740 - acc: 0.9890 - val_loss: 0.0802 - val_acc: 0.9825\n",
      "Epoch 126/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0740 - acc: 0.9890 - val_loss: 0.0801 - val_acc: 0.9825\n",
      "Epoch 127/200\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.0739 - acc: 0.9890 - val_loss: 0.0800 - val_acc: 0.9825\n",
      "Epoch 128/200\n",
      "455/455 [==============================] - 0s 41us/step - loss: 0.0738 - acc: 0.9890 - val_loss: 0.0800 - val_acc: 0.9825\n",
      "Epoch 129/200\n",
      "455/455 [==============================] - 0s 41us/step - loss: 0.0737 - acc: 0.9890 - val_loss: 0.0799 - val_acc: 0.9825\n",
      "Epoch 130/200\n",
      "455/455 [==============================] - 0s 39us/step - loss: 0.0737 - acc: 0.9890 - val_loss: 0.0798 - val_acc: 0.9825\n",
      "Epoch 131/200\n",
      "455/455 [==============================] - 0s 42us/step - loss: 0.0736 - acc: 0.9890 - val_loss: 0.0798 - val_acc: 0.9825\n",
      "Epoch 132/200\n",
      "455/455 [==============================] - 0s 40us/step - loss: 0.0735 - acc: 0.9890 - val_loss: 0.0797 - val_acc: 0.9825\n",
      "Epoch 133/200\n",
      "455/455 [==============================] - 0s 40us/step - loss: 0.0734 - acc: 0.9890 - val_loss: 0.0797 - val_acc: 0.9825\n",
      "Epoch 134/200\n",
      "455/455 [==============================] - 0s 39us/step - loss: 0.0734 - acc: 0.9890 - val_loss: 0.0797 - val_acc: 0.9825\n",
      "Epoch 135/200\n",
      "455/455 [==============================] - 0s 39us/step - loss: 0.0733 - acc: 0.9890 - val_loss: 0.0797 - val_acc: 0.9825\n",
      "Epoch 136/200\n",
      "455/455 [==============================] - 0s 35us/step - loss: 0.0732 - acc: 0.9890 - val_loss: 0.0796 - val_acc: 0.9825\n",
      "Epoch 137/200\n",
      "455/455 [==============================] - 0s 36us/step - loss: 0.0732 - acc: 0.9890 - val_loss: 0.0795 - val_acc: 0.9825\n",
      "Epoch 138/200\n",
      "455/455 [==============================] - 0s 35us/step - loss: 0.0731 - acc: 0.9890 - val_loss: 0.0795 - val_acc: 0.9825\n",
      "Epoch 139/200\n",
      "455/455 [==============================] - 0s 42us/step - loss: 0.0730 - acc: 0.9890 - val_loss: 0.0794 - val_acc: 0.9825\n",
      "Epoch 140/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0729 - acc: 0.9890 - val_loss: 0.0793 - val_acc: 0.9825\n",
      "Epoch 141/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0729 - acc: 0.9890 - val_loss: 0.0793 - val_acc: 0.9825\n",
      "Epoch 142/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0728 - acc: 0.9890 - val_loss: 0.0792 - val_acc: 0.9825\n",
      "Epoch 143/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0727 - acc: 0.9890 - val_loss: 0.0792 - val_acc: 0.9825\n",
      "Epoch 144/200\n",
      "455/455 [==============================] - 0s 43us/step - loss: 0.0726 - acc: 0.9890 - val_loss: 0.0791 - val_acc: 0.9825\n",
      "Epoch 145/200\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.0726 - acc: 0.9890 - val_loss: 0.0790 - val_acc: 0.9825\n",
      "Epoch 146/200\n",
      "455/455 [==============================] - 0s 40us/step - loss: 0.0725 - acc: 0.9890 - val_loss: 0.0790 - val_acc: 0.9825\n",
      "Epoch 147/200\n",
      "455/455 [==============================] - 0s 40us/step - loss: 0.0724 - acc: 0.9890 - val_loss: 0.0789 - val_acc: 0.9825\n",
      "Epoch 148/200\n",
      "455/455 [==============================] - 0s 35us/step - loss: 0.0724 - acc: 0.9890 - val_loss: 0.0789 - val_acc: 0.9825\n",
      "Epoch 149/200\n",
      "455/455 [==============================] - 0s 35us/step - loss: 0.0723 - acc: 0.9890 - val_loss: 0.0788 - val_acc: 0.9825\n",
      "Epoch 150/200\n",
      "455/455 [==============================] - 0s 38us/step - loss: 0.0722 - acc: 0.9890 - val_loss: 0.0787 - val_acc: 0.9825\n",
      "Epoch 151/200\n",
      "455/455 [==============================] - 0s 35us/step - loss: 0.0722 - acc: 0.9890 - val_loss: 0.0787 - val_acc: 0.9825\n",
      "Epoch 152/200\n",
      "455/455 [==============================] - 0s 35us/step - loss: 0.0721 - acc: 0.9890 - val_loss: 0.0786 - val_acc: 0.9825\n",
      "Epoch 153/200\n",
      "455/455 [==============================] - 0s 41us/step - loss: 0.0720 - acc: 0.9890 - val_loss: 0.0786 - val_acc: 0.9825\n",
      "Epoch 154/200\n",
      "455/455 [==============================] - 0s 42us/step - loss: 0.0719 - acc: 0.9890 - val_loss: 0.0785 - val_acc: 0.9825\n",
      "Epoch 155/200\n",
      "455/455 [==============================] - 0s 43us/step - loss: 0.0719 - acc: 0.9890 - val_loss: 0.0785 - val_acc: 0.9825\n",
      "Epoch 156/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0718 - acc: 0.9890 - val_loss: 0.0784 - val_acc: 0.9825\n",
      "Epoch 157/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0717 - acc: 0.9890 - val_loss: 0.0784 - val_acc: 0.9825\n",
      "Epoch 158/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0717 - acc: 0.9890 - val_loss: 0.0783 - val_acc: 0.9825\n",
      "Epoch 159/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0716 - acc: 0.9890 - val_loss: 0.0782 - val_acc: 0.9825\n",
      "Epoch 160/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0715 - acc: 0.9890 - val_loss: 0.0782 - val_acc: 0.9825\n",
      "Epoch 161/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0715 - acc: 0.9890 - val_loss: 0.0781 - val_acc: 0.9825\n",
      "Epoch 162/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0714 - acc: 0.9890 - val_loss: 0.0781 - val_acc: 0.9825\n",
      "Epoch 163/200\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.0713 - acc: 0.9890 - val_loss: 0.0781 - val_acc: 0.9825\n",
      "Epoch 164/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0713 - acc: 0.9890 - val_loss: 0.0781 - val_acc: 0.9825\n",
      "Epoch 165/200\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0712 - acc: 0.9890 - val_loss: 0.0781 - val_acc: 0.9825\n",
      "Epoch 166/200\n",
      "455/455 [==============================] - 0s 67us/step - loss: 0.0711 - acc: 0.9890 - val_loss: 0.0780 - val_acc: 0.9825\n",
      "Epoch 167/200\n",
      "455/455 [==============================] - 0s 56us/step - loss: 0.0711 - acc: 0.9890 - val_loss: 0.0780 - val_acc: 0.9825\n",
      "Epoch 168/200\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0710 - acc: 0.9890 - val_loss: 0.0779 - val_acc: 0.9825\n",
      "Epoch 169/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0709 - acc: 0.9890 - val_loss: 0.0778 - val_acc: 0.9825\n",
      "Epoch 170/200\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0709 - acc: 0.9890 - val_loss: 0.0778 - val_acc: 0.9825\n",
      "Epoch 171/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0708 - acc: 0.9890 - val_loss: 0.0778 - val_acc: 0.9825\n",
      "Epoch 172/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0707 - acc: 0.9890 - val_loss: 0.0777 - val_acc: 0.9825\n",
      "Epoch 173/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0707 - acc: 0.9890 - val_loss: 0.0777 - val_acc: 0.9825\n",
      "Epoch 174/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0706 - acc: 0.9890 - val_loss: 0.0776 - val_acc: 0.9825\n",
      "Epoch 175/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0705 - acc: 0.9890 - val_loss: 0.0776 - val_acc: 0.9825\n",
      "Epoch 176/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0705 - acc: 0.9890 - val_loss: 0.0775 - val_acc: 0.9825\n",
      "Epoch 177/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0704 - acc: 0.9890 - val_loss: 0.0775 - val_acc: 0.9825\n",
      "Epoch 178/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0703 - acc: 0.9890 - val_loss: 0.0775 - val_acc: 0.9825\n",
      "Epoch 179/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0703 - acc: 0.9890 - val_loss: 0.0774 - val_acc: 0.9825\n",
      "Epoch 180/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0702 - acc: 0.9890 - val_loss: 0.0774 - val_acc: 0.9825\n",
      "Epoch 181/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0702 - acc: 0.9890 - val_loss: 0.0774 - val_acc: 0.9825\n",
      "Epoch 182/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0701 - acc: 0.9890 - val_loss: 0.0773 - val_acc: 0.9825\n",
      "Epoch 183/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0700 - acc: 0.9890 - val_loss: 0.0773 - val_acc: 0.9825\n",
      "Epoch 184/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0700 - acc: 0.9890 - val_loss: 0.0773 - val_acc: 0.9825\n",
      "Epoch 185/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0699 - acc: 0.9890 - val_loss: 0.0773 - val_acc: 0.9825\n",
      "Epoch 186/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0698 - acc: 0.9890 - val_loss: 0.0772 - val_acc: 0.9825\n",
      "Epoch 187/200\n",
      "455/455 [==============================] - 0s 44us/step - loss: 0.0698 - acc: 0.9890 - val_loss: 0.0772 - val_acc: 0.9825\n",
      "Epoch 188/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0697 - acc: 0.9890 - val_loss: 0.0772 - val_acc: 0.9825\n",
      "Epoch 189/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0697 - acc: 0.9890 - val_loss: 0.0771 - val_acc: 0.9825\n",
      "Epoch 190/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0696 - acc: 0.9890 - val_loss: 0.0771 - val_acc: 0.9825\n",
      "Epoch 191/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0695 - acc: 0.9890 - val_loss: 0.0771 - val_acc: 0.9825\n",
      "Epoch 192/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0695 - acc: 0.9890 - val_loss: 0.0770 - val_acc: 0.9825\n",
      "Epoch 193/200\n",
      "455/455 [==============================] - 0s 48us/step - loss: 0.0694 - acc: 0.9890 - val_loss: 0.0770 - val_acc: 0.9825\n",
      "Epoch 194/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0694 - acc: 0.9890 - val_loss: 0.0769 - val_acc: 0.9825\n",
      "Epoch 195/200\n",
      "455/455 [==============================] - 0s 45us/step - loss: 0.0693 - acc: 0.9890 - val_loss: 0.0769 - val_acc: 0.9825\n",
      "Epoch 196/200\n",
      "455/455 [==============================] - 0s 47us/step - loss: 0.0692 - acc: 0.9890 - val_loss: 0.0769 - val_acc: 0.9825\n",
      "Epoch 197/200\n",
      "455/455 [==============================] - 0s 50us/step - loss: 0.0692 - acc: 0.9890 - val_loss: 0.0768 - val_acc: 0.9825\n",
      "Epoch 198/200\n",
      "455/455 [==============================] - 0s 46us/step - loss: 0.0691 - acc: 0.9890 - val_loss: 0.0768 - val_acc: 0.9825\n",
      "Epoch 199/200\n",
      "455/455 [==============================] - 0s 51us/step - loss: 0.0691 - acc: 0.9890 - val_loss: 0.0767 - val_acc: 0.9825\n",
      "Epoch 200/200\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0690 - acc: 0.9890 - val_loss: 0.0767 - val_acc: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe29e5b4520>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=200, batch_size=128, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s 102us/step\n",
      "train score 0.9582417582417583\n",
      "114/114 [==============================] - 0s 122us/step\n",
      "test score 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "print(\"train score\", model.evaluate(x_train, y_train)[1])\n",
    "print(\"test score\", model.evaluate(x_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe2a0f19640>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2a0dd1c10>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(model.history.history['acc'], label=\"acc\")\n",
    "plt.plot(model.history.history['loss'], label=\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
